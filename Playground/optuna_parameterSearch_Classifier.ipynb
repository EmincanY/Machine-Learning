{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=34):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a global Config class which holds common configuration.\n",
    "\n",
    "We will also define a configuration class for each of the models that will be used in the ensemble. These classes will hold the default hyperparameters, static parameters (parameters that are not tuned), extra parameters to pass to .fit() etc., for the respective models.\n",
    "\n",
    "Additionally, we will have a mapping between model names and their configuration classes so that we can get to them easily.\n",
    "\n",
    "Note: The get_fit_params() method is required in the configuration classes of the models since there are subtle differences between what the .fit() method of each model accepts. For example, LogisticRegression.fit() has no parameter called eval_set while XGBClassifier.fit() does. LogisticRegression takes the verbose argument in the initializer while XGBoost takes it in .fit(). XGBoost, since version 1.6.0, has moved arguments like callbacks and eval_metric to the initializer while LightGBM continues to use them in .fit(). These differences are reconciled by dividing the arguments appropriately between STATIC_PARAMS and the output of get_fit_params()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"C:/Users/Emincan/Desktop/Playground/\"\n",
    "    L1_N_TRIALS = 100\n",
    "    L2_N_TRIALS = 20\n",
    "    N_JOBS = 2\n",
    "    \n",
    "    # Map internal identifier to human-friendly name\n",
    "    MODELS = {\n",
    "        \"lr\": \"Logisitc Regression\",\n",
    "        \"ada\": \"AdaBoost\",\n",
    "        \"rf\": \"Random Forest\",\n",
    "        \"xgb\": \"XGBoost\",\n",
    "        \"lgb\": \"LightGBM\",\n",
    "        \"cb\": \"CatBoost\",\n",
    "        \"gbr\": \"GradientBoosting\",\n",
    "        \"hgbr\": \"HistGradientBoosting\",\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def filepath(cls, filename):\n",
    "        return os.path.join(cls.DATA_DIR, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Logistic Regression\n",
    "class LRConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"tol\": 1e-4,\n",
    "        \"C\": 1.0,\n",
    "        \"solver\": \"lbfgs\",\n",
    "    }\n",
    "    STATIC_PARAMS = {\n",
    "        \"max_iter\": 1000,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "    \n",
    "    USE_PRUNER = False\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for AdaBoost\n",
    "class AdaConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"base_estimator\": None,\n",
    "        \"n_estimators\": 50,\n",
    "        \"learning_rate\": 1.0,\n",
    "        \n",
    "    }\n",
    "    STATIC_PARAMS = {}\n",
    "    \n",
    "    USE_PRUNER = False\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Random Forest\n",
    "class RFConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"bootstrap\": True,\n",
    "        \"ccp_alpha\": 0.0,\n",
    "        \"max_samples\": None,\n",
    "    }\n",
    "\n",
    "    STATIC_PARAMS = {\n",
    "        \"n_jobs\": Config.N_JOBS,\n",
    "    }\n",
    "    \n",
    "    USE_PRUNER = False\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for XGBoost\n",
    "class XGBConfig:\n",
    "    EVAL_METRIC = \"logloss\"\n",
    "    \n",
    "    DEFAULT_VALUES = {\n",
    "        \"max_depth\": 6,\n",
    "        \"n_estimators\": 100,\n",
    "        \"alpha\": 0.0,\n",
    "        \"lambda\": 1.0,\n",
    "        \"learning_rate\": 0.3,\n",
    "        \"colsample_bytree\": 1.0,\n",
    "        \"colsample_bylevel\": 1.0,\n",
    "        \"min_child_weight\": 1.0,\n",
    "        \"sampling_method\": \"uniform\",\n",
    "        \"early_stopping_rounds\": None,\n",
    "    }\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"use_label_encoder\": False,\n",
    "        \"n_jobs\": Config.N_JOBS,\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"max_bin\": 1024,\n",
    "        \"eval_metric\": EVAL_METRIC,\n",
    "    }\n",
    "    \n",
    "    USE_PRUNER = True\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {\n",
    "            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n",
    "            \"verbose\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for LightGBM\n",
    "class LGBConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"num_leaves\": 31,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"reg_alpha\": 0.0,\n",
    "        \"reg_lambda\": 0.0,\n",
    "        \"min_child_samples\": 20,\n",
    "        \"subsample_for_bin\": 200000,\n",
    "    }\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "        \"n_jobs\": Config.N_JOBS,\n",
    "        \"verbose\": -1,\n",
    "        \"objective\": \"binary\",\n",
    "    }\n",
    "    \n",
    "    USE_PRUNER = True\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        # To suppress training output\n",
    "        callbacks = params.get(\"callbacks\", []) + [lgb.log_evaluation(period=0)]\n",
    "        return {\n",
    "            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"callbacks\": callbacks,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for LightGBM\n",
    "class LGBConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"num_leaves\": 31,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 100,\n",
    "        \"reg_alpha\": 0.0,\n",
    "        \"reg_lambda\": 0.0,\n",
    "        \"min_child_samples\": 20,\n",
    "        \"subsample_for_bin\": 200000,\n",
    "    }\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "        \"n_jobs\": Config.N_JOBS,\n",
    "        \"verbose\": -1,\n",
    "        \"objective\": \"binary\",\n",
    "    }\n",
    "    \n",
    "    USE_PRUNER = True\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        # To suppress training output\n",
    "        callbacks = params.get(\"callbacks\", []) + [lgb.log_evaluation(period=0)]\n",
    "        return {\n",
    "            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"callbacks\": callbacks,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"depth\": 6,\n",
    "        \"l2_leaf_reg\": 3,\n",
    "        \"border_count\": 32,\n",
    "        \"thread_count\": -1,\n",
    "        \"random_seed\": 42,\n",
    "    }\n",
    "\n",
    "    STATIC_PARAMS = {\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"Logloss\",\n",
    "        \"task_type\": \"CPU\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    USE_PRUNER = True\n",
    "\n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {\n",
    "            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n",
    "            \"use_best_model\": True,\n",
    "            \"early_stopping_rounds\": 100,\n",
    "            \"verbose_eval\": False,\n",
    "            \"plot\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBRConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 3,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"subsample\": 1.0,\n",
    "        \"max_features\": None,\n",
    "        \"random_state\": None,\n",
    "        \"alpha\": 0.9,\n",
    "    }\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "        \"loss\": \"ls\",\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {\n",
    "            \"verbose\": 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGBRConfig:\n",
    "    DEFAULT_VALUES = {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_iter\": 100,\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_leaf\": 20,\n",
    "        \"l2_regularization\": 0.0,\n",
    "        \"max_leaf_nodes\": 31,\n",
    "        \"random_state\": None,\n",
    "    }\n",
    "\n",
    "    STATIC_PARAMS = {\n",
    "        \"loss\": \"least_squares\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n",
    "        return {\n",
    "            \"verbose\": 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MAP = {\n",
    "    \"lr\": LRConfig,\n",
    "    \"ada\": AdaConfig,\n",
    "    \"rf\": RFConfig,\n",
    "    \"xgb\": XGBConfig,\n",
    "    \"lgb\": LGBConfig,\n",
    "    \"cb\" : CBConfig,\n",
    "    \"gbr\" : GBRConfig,\n",
    "    \"hgbr\" : HGBRConfig\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(Config.filepath('train.csv'))\n",
    "test = pd.read_csv(Config.filepath('test.csv'))\n",
    "original = pd.read_csv(Config.filepath('CrabAgePrediction.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>1.5250</td>\n",
       "      <td>1.1750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>28.973189</td>\n",
       "      <td>12.728926</td>\n",
       "      <td>6.647958</td>\n",
       "      <td>8.348928</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>10.418441</td>\n",
       "      <td>4.521745</td>\n",
       "      <td>2.324659</td>\n",
       "      <td>3.401940</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>1.3875</td>\n",
       "      <td>1.1125</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>24.777463</td>\n",
       "      <td>11.339800</td>\n",
       "      <td>5.556502</td>\n",
       "      <td>6.662133</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>1.4125</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>50.660556</td>\n",
       "      <td>20.354941</td>\n",
       "      <td>10.991839</td>\n",
       "      <td>14.996885</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>1.0125</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>23.289114</td>\n",
       "      <td>11.977664</td>\n",
       "      <td>4.507570</td>\n",
       "      <td>5.953395</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id Sex  Length  Diameter  Height     Weight  Shucked Weight   \n",
       "0   0   I  1.5250    1.1750  0.3750  28.973189       12.728926  \\\n",
       "1   1   I  1.1000    0.8250  0.2750  10.418441        4.521745   \n",
       "2   2   M  1.3875    1.1125  0.3750  24.777463       11.339800   \n",
       "3   3   F  1.7000    1.4125  0.5000  50.660556       20.354941   \n",
       "4   4   I  1.2500    1.0125  0.3375  23.289114       11.977664   \n",
       "\n",
       "   Viscera Weight  Shell Weight  Age  \n",
       "0        6.647958      8.348928    9  \n",
       "1        2.324659      3.401940    8  \n",
       "2        5.556502      6.662133    9  \n",
       "3       10.991839     14.996885   11  \n",
       "4        4.507570      5.953395    8  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74051</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>8.618248</td>\n",
       "      <td>3.657085</td>\n",
       "      <td>1.729319</td>\n",
       "      <td>2.721552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74052</td>\n",
       "      <td>I</td>\n",
       "      <td>1.1625</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>15.507176</td>\n",
       "      <td>7.030676</td>\n",
       "      <td>3.246018</td>\n",
       "      <td>3.968930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74053</td>\n",
       "      <td>F</td>\n",
       "      <td>1.2875</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>14.571643</td>\n",
       "      <td>5.556502</td>\n",
       "      <td>3.883882</td>\n",
       "      <td>4.819415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74054</td>\n",
       "      <td>F</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>28.377849</td>\n",
       "      <td>13.380964</td>\n",
       "      <td>6.548735</td>\n",
       "      <td>7.030676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74055</td>\n",
       "      <td>I</td>\n",
       "      <td>1.1125</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>11.765042</td>\n",
       "      <td>5.528153</td>\n",
       "      <td>2.466407</td>\n",
       "      <td>3.331066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id Sex  Length  Diameter  Height     Weight  Shucked Weight   \n",
       "0  74051   I  1.0500    0.7625  0.2750   8.618248        3.657085  \\\n",
       "1  74052   I  1.1625    0.8875  0.2750  15.507176        7.030676   \n",
       "2  74053   F  1.2875    0.9875  0.3250  14.571643        5.556502   \n",
       "3  74054   F  1.5500    0.9875  0.3875  28.377849       13.380964   \n",
       "4  74055   I  1.1125    0.8500  0.2625  11.765042        5.528153   \n",
       "\n",
       "   Viscera Weight  Shell Weight  \n",
       "0        1.729319      2.721552  \n",
       "1        3.246018      3.968930  \n",
       "2        3.883882      4.819415  \n",
       "3        6.548735      7.030676  \n",
       "4        2.466407      3.331066  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAP = {\n",
    "    \"lr\": LogisticRegression,\n",
    "    \"ada\": AdaBoostClassifier,\n",
    "    \"rf\": RandomForestClassifier,\n",
    "    \"xgb\": xgb.XGBClassifier,\n",
    "    \"lgb\": lgb.LGBMClassifier,\n",
    "    \"cat\": CatBoostClassifier,\n",
    "    \"gb\" : GradientBoostingClassifier,\n",
    "    \"hgb\": HistGradientBoostingClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainingLoop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train() function will be used as a generic training loop. It takes the training and test dataframes, a string identifier for the model that is being trained, the hyperparameters for the model and an optional verbosity argument. \n",
    "\n",
    "In the end, it returns the predictions for the training set, the test set and the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_df, test_df, model, params, verbose=True):\n",
    "    # Create copies so that original datatsets do not change\n",
    "    df = train_df.copy()\n",
    "    test = test_df.drop(\"PassengerId\", axis=1)\n",
    "    \n",
    "    df[\"preds\"] = pd.NA\n",
    "    \n",
    "    drop = [\"Transported\", \"preds\", \"kfold\"]\n",
    "    \n",
    "    # Get the initializer class and the configuration class\n",
    "    klass = MODEL_MAP[model]\n",
    "    config = CONFIG_MAP[model]\n",
    "    \n",
    "    # Default values in the config class are for tuned parameters\n",
    "    # So, only those are filtered from params\n",
    "    # This is mainly added for AdaBoost, since it has a slightly different objective\n",
    "    params = {k: v for k, v in params.items() if k in config.DEFAULT_VALUES}\n",
    "    \n",
    "    # Add default values for parameters not defined in params\n",
    "    params.update({k: v for k, v in config.DEFAULT_VALUES.items() if k not in params})\n",
    "    \n",
    "    # Add static params - Parameters that are not tuned\n",
    "    params.update(config.STATIC_PARAMS)\n",
    "    \n",
    "    # For storing total accuracy across folds for averaging\n",
    "    total_acc = 0.0\n",
    "    \n",
    "    # Empty list for storing test predictions in each fold\n",
    "    test_preds = []\n",
    "    \n",
    "    for fold in range(5):\n",
    "        train = df[df[\"kfold\"] != fold]\n",
    "        \n",
    "        # Get training features and labels\n",
    "        y_train = train[\"Transported\"]\n",
    "        X_train = train.drop(drop, axis=1)\n",
    "        \n",
    "        val = df[df[\"kfold\"] == fold]\n",
    "        \n",
    "        # Get validation features and labels\n",
    "        y_val = val[\"Transported\"]\n",
    "        X_val = val.drop(drop, axis=1)\n",
    "        \n",
    "        # Initialize model\n",
    "        clf = klass(**params)\n",
    "        \n",
    "        # Get parameters for .fit() other than X and y\n",
    "        fit_params = config.get_fit_params(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "        # Train model on training set\n",
    "        clf.fit(\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            **fit_params,\n",
    "        )\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        val_pred = clf.predict(X_val)\n",
    "        acc = metrics.accuracy_score(y_val, val_pred)\n",
    "        \n",
    "        # Report accuracy if verbose is True\n",
    "        if verbose is True:\n",
    "            print(f\"\\tFold {fold + 1} - Accuracy = {acc: .4f}\")\n",
    "        \n",
    "        # Add to total accuracy\n",
    "        total_acc += acc\n",
    "        \n",
    "        # Make predictions on validation set again\n",
    "        # But this time in terms of probabilities\n",
    "        # And store in the df\n",
    "        # These will be used in the meta model\n",
    "        df.loc[val.index, \"preds\"] = clf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Get the test predictions for this fold in terms of probability\n",
    "        test_preds.append(clf.predict_proba(test)[:, 1])\n",
    "        \n",
    "    acc = total_acc / 5\n",
    "    \n",
    "    if verbose is True:\n",
    "        print(f\"\\tOverall accuracy = {acc: .4f}\")   \n",
    "    \n",
    "    # Calculate final test predictions\n",
    "    # These will be used in the meta model\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    test_preds = test_preds.mean(axis=0)\n",
    "    \n",
    "    # Return val preds, test preds and overall accuracy\n",
    "    return df[\"preds\"].values, test_preds, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below will act as the objective functions that should be used for each of the models. \n",
    "\n",
    "These take the Optuna trial, the training dataframe and the test dataframe as arguments. They first use Optuna to get a dictionary of parameters and then call the train() function with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for Logistic Regression\n",
    "def lr_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-4, log=True),\n",
    "        \"C\": trial.suggest_float(\"C\", 0.5, 2.0, log=True),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]),\n",
    "    }\n",
    "    \n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"lr\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for AdaBoost\n",
    "def adaboost_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 5.0, log=True),\n",
    "    }\n",
    "    \n",
    "    tune_estimator = trial.suggest_categorical(\"tune_estimator\", [True, False])\n",
    "    \n",
    "    if tune_estimator:\n",
    "        # Parameters for the Decision Tree in AdaBoost\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10, log=True)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5, log=True)\n",
    "        ccp_alpha = trial.suggest_float(\"ccp_alpha\", 0.01, 1.0, log=True)\n",
    "        \n",
    "        params[\"base_estimator\"] = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            ccp_alpha=ccp_alpha,\n",
    "        )\n",
    "    \n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"ada\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objectuve for Random Forest\n",
    "def rf_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 50),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_plit\", 2, 10, log=True),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5, log=True),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.01, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    if params[\"bootstrap\"] is True:\n",
    "        params[\"max_samples\"] = trial.suggest_float(\"max_samples\", 0.01, 1.0, log=True)\n",
    "    \n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"rf\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for XGBoost\n",
    "def xgb_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 11),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 500),\n",
    "        \"alpha\": trial.suggest_uniform(\"alpha\", 0.0, 5.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1.0, 5.0, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.8, log=True),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.2, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_uniform(\"min_child_weight\", 1, 100),\n",
    "        \"sampling_method\": trial.suggest_categorical(\"sampling_method\", [\"uniform\", \"gradient_based\"]),\n",
    "        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 5, 20, step=5)\n",
    "    }\n",
    "    \n",
    "    obs_k = f\"validation_1-{XGBConfig.EVAL_METRIC}\"\n",
    "    params[\"callbacks\"] = [optuna.integration.XGBoostPruningCallback(trial, obs_k)]\n",
    "    \n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"xgb\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for LightGBM\n",
    "def lgb_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 100, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 50, log=True),\n",
    "        \"subsample_for_bin\": trial.suggest_int(\"subsample_for_bin\", 2000, 8000),\n",
    "    }\n",
    "    \n",
    "    params[\"callbacks\"] = [optuna.integration.LightGBMPruningCallback(trial, \"logloss\", \"valid_1\")]\n",
    "    \n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"lgb\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for CatBoost\n",
    "def cb_objective(trial, train_df, test_df):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.1, 10.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "    }\n",
    "\n",
    "\n",
    "    params['callbacks'] = optuna.integration.CatBoostPruningCallback(trial, \"Logloss\", 'valid_1')\n",
    "\n",
    "\n",
    "    _, _, acc = train(df=train_df, test_df=test_df, model=\"cb\", params=params, verbose=False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Objective for GradientBoosting\n",
    "# def cb_objective(trial, train_df, test_df):\n",
    "#     params = {\n",
    "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "#         \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "#         \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "#     }\n",
    "\n",
    "\n",
    "#     params['callbacks'] = optuna.integration.CatBoostPruningCallback(trial, \"Logloss\", 'valid_1')\n",
    "\n",
    "\n",
    "#     _, _, acc = train(df=train_df, test_df=test_df, model=\"cb\", params=params, verbose=False)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE_MAP = {\n",
    "    \"lr\": lr_objective,\n",
    "    \"ada\": adaboost_objective,\n",
    "    \"rf\": rf_objective,\n",
    "    \"xgb\": xgb_objective,\n",
    "    \"lgb\": lgb_objective,\n",
    "    \"cb\" : cb_objective\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function hyperparameter_search() finds the best hyperparameters for the given model by utilizing the proper objective function. It takes the training dataframe, test dataframe and the string identifier of the model for which hyperparameters are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(train_df, test_df, model, n_trials=Config.L1_N_TRIALS):\n",
    "    # Turn off verbose output\n",
    "    v = optuna.logging.get_verbosity()\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    objective = OBJECTIVE_MAP[model]\n",
    "    objective = functools.partial(objective, train_df=train_df, test_df=test_df)\n",
    "    \n",
    "    # Check if pruning is required\n",
    "    pruner = optuna.pruners.HyperbandPruner() if CONFIG_MAP[model].USE_PRUNER is True else None\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=pruner,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Restore verbosity level\n",
    "    optuna.logging.set_verbosity(v)\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for LGBMClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is unique from other boosting algorithms in that it supports categorical features out of the box. \n",
    "\n",
    "We only need to make sure they are label encoded and have their datatype as category. \n",
    "\n",
    "The function below takes the two datasets and label encodes all the one-hot encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_datasets(train_df, test_df):\n",
    "    # Make copies so that original datasets remain unchanged\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Drop Transported and kfold\n",
    "    drop = [\"Transported\", \"kfold\"]\n",
    "    dropped = train_df[drop].values\n",
    "    train_df = train_df.drop(drop, axis=1)\n",
    "    \n",
    "    # Drop PassengerId\n",
    "    passenger_id = test_df[\"PassengerId\"].values\n",
    "    test_df = test_df.drop(\"PassengerId\", axis=1)\n",
    "    \n",
    "    # Add suffix to index and store indices\n",
    "    # So that the dataframes can be merged and split\n",
    "    train_df = train_df.rename(\"train_{}\".format)\n",
    "    test_df = test_df.rename(\"test_{}\".format)\n",
    "    \n",
    "    tr_idx = train_df.index\n",
    "    te_idx = test_df.index\n",
    "    \n",
    "    # Merge\n",
    "    df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    oh_cols = [\"CabinDeck\", \"HomePlanet\", \"Destination\", \"GroupSize\"]\n",
    "    \n",
    "    for oh_col in oh_cols:\n",
    "        # Get all columns associated with the one-hot column\n",
    "        columns = [column for column in df.columns if column.startswith(f\"{oh_col}_\")]\n",
    "        \n",
    "        # .idxmax() returns that column name which has the maximum value in the row\n",
    "        values = df[columns].idxmax(axis=1)\n",
    "        \n",
    "        # Get all levels and make a mapping from level to index\n",
    "        levels = values.value_counts().index\n",
    "        mapping = {level: idx for idx, level in enumerate(levels)}\n",
    "        \n",
    "        # Add column with the mapping and specify type as category\n",
    "        df[oh_col] = values.map(mapping).astype(\"category\")\n",
    "        \n",
    "        # Drop one-hot columns\n",
    "        df = df.drop(columns, axis=1)\n",
    "        \n",
    "    # Make sure other categorical features have the correct type\n",
    "    missing = (col for col in df.columns if col.endswith(\"_missing\"))\n",
    "    others = [\"CryoSleep\", \"VIP\", \"Alone\", \"CabinNum\", \"GroupId\", *missing]\n",
    "    df[others] = df[others].astype(\"category\")\n",
    "        \n",
    "    # Split and add dropped columns\n",
    "    train_df = df.loc[tr_idx, :]\n",
    "    train_df[drop] = dropped\n",
    "    \n",
    "    test_df = df.loc[te_idx, :]\n",
    "    test_df[\"PassengerId\"] = passenger_id\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ensemble class will implement all the logic required for stacking. It takes the training dataframe, the test dataframe and an optional list of strings which specifies the models that should be excluded from the ensemble. It has the following methods:\n",
    "\n",
    "fit_level_one_models(): This method fits all the different models (Logistic Regression, XGBoost, etc.) on the original training data and also creates the training set and test sets for the meta-classifier that will generate the final predictions.\n",
    "fit_level_two_model(): This method fits a logistic regression model on the dataset generated by fit_level_one_models() and gets the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, train_df, test_df, exclude=None):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        models = Config.MODELS.keys()\n",
    "        \n",
    "        # Exclude models\n",
    "        if exclude is not None:\n",
    "            models = models - exclude\n",
    "            \n",
    "        self.models = list(models)\n",
    "        \n",
    "        # Create empty dataframe that will store\n",
    "        # The training set for the level 2 model\n",
    "        columns = [f\"{m}_preds\" for m in self.models]\n",
    "        extra_cols = [\"Transported\", \"kfold\"]\n",
    "        \n",
    "        meta_train_df = pd.DataFrame(columns=columns + extra_cols)\n",
    "        meta_train_df[extra_cols] = train_df[extra_cols]\n",
    "        \n",
    "        self.meta_train_df = meta_train_df\n",
    "        \n",
    "        # Create empty dataframe that will store\n",
    "        # The test set for the level 2 model\n",
    "        meta_test_df = pd.DataFrame(columns=[\"PassengerId\"] + columns)\n",
    "        meta_test_df[\"PassengerId\"] = test_df[\"PassengerId\"]\n",
    "        \n",
    "        self.meta_test_df = meta_test_df\n",
    "        \n",
    "    def fit_level_one_models(self):\n",
    "        print(\"Training level 1 models...\")\n",
    "        \n",
    "        for model in self.models:\n",
    "            if model == \"lgb\":\n",
    "                # Modify dataset for LGBMClassifier\n",
    "                train_df, test_df = lgb_datasets(self.train_df, self.test_df)\n",
    "            else:\n",
    "                train_df, test_df = self.train_df, self.test_df\n",
    "            \n",
    "            print(f\"{Config.MODELS[model]}:\")\n",
    "            \n",
    "            print(\"\\tFinding optimal hyperparameters using Optuna...\")\n",
    "            params = hyperparameter_search(\n",
    "                train_df=train_df, test_df=test_df, model=model\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n\\tBest params: {params}\\n\")\n",
    "            \n",
    "            print(\"\\tTraining model with optimal parameters...\\n\")\n",
    "            val_preds, test_preds, acc = train(\n",
    "                df=train_df,\n",
    "                test_df=test_df,\n",
    "                model=model,\n",
    "                params=params\n",
    "            )\n",
    "            \n",
    "            print(\"\\tDone!\\n\")\n",
    "            \n",
    "            # Add predictions to the datasets for the level 2 model\n",
    "            self.meta_train_df[f\"{model}_preds\"] = val_preds\n",
    "            self.meta_test_df[f\"{model}_preds\"] = test_preds\n",
    "            \n",
    "    def fit_level_two_model(self):    \n",
    "        print(\"Training a Logistic Regression model as level 2 model...\")\n",
    "        \n",
    "        train_df = self.meta_train_df\n",
    "        test_df = self.meta_test_df\n",
    "        \n",
    "        print(\"\\tFinding optimal hyperparameters using Optuna...\")\n",
    "        params = hyperparameter_search(\n",
    "            train_df=train_df,\n",
    "            test_df=test_df,\n",
    "            model=\"lr\",\n",
    "            n_trials=Config.L2_N_TRIALS,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n\\tBest params: {params}\\n\")\n",
    "\n",
    "        print(\"\\tTraining model with optimal parameters...\\n\")\n",
    "        \n",
    "        _, test_preds, _ = train(\n",
    "            df=train_df,\n",
    "            test_df=test_df,\n",
    "            model=\"lr\",\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "        print(\"\\tDone!\")\n",
    "        \n",
    "        self.meta_test_df[\"Transported\"] = test_preds >= 0.5\n",
    "        \n",
    "        return self.meta_test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use the Ensemble class to train our ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Initialize the ensemble\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ensemble \u001b[39m=\u001b[39m Ensemble(train, test)\n\u001b[0;32m      3\u001b[0m ensemble\u001b[39m.\u001b[39mmodels\n",
      "Cell \u001b[1;32mIn[41], line 20\u001b[0m, in \u001b[0;36mEnsemble.__init__\u001b[1;34m(self, train_df, test_df, exclude)\u001b[0m\n\u001b[0;32m     17\u001b[0m extra_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mTransported\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkfold\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m meta_train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mcolumns \u001b[39m+\u001b[39m extra_cols)\n\u001b[1;32m---> 20\u001b[0m meta_train_df[extra_cols] \u001b[39m=\u001b[39m train_df[extra_cols]\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_train_df \u001b[39m=\u001b[39m meta_train_df\n\u001b[0;32m     24\u001b[0m \u001b[39m# Create empty dataframe that will store\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# The test set for the level 2 model\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Initialize the ensemble\n",
    "ensemble = Ensemble(train, test)\n",
    "ensemble.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the level one models\n",
    "ensemble.fit_level_one_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated training set for level 2 model\n",
    "ensemble.meta_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated test set for level 2 model\n",
    "ensemble.meta_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the level 2 model\n",
    "test_predictions = ensemble.fit_level_two_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
